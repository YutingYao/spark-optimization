{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle Operation and Shuffle Service\n",
    "\n",
    "![footer_logo_new](images/logo_new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Operation\n",
    "\n",
    "Re-distribution of the dataset is the primary goal of shuffling operation. \n",
    "The need to re-distribute the dataset could be there in order to:\n",
    "\n",
    "#### Increase or Decrease the number of data partitions\n",
    "Since a data partition represents the quantum of data to be processed together by a single Spark Task, there could be situations:\n",
    "- existing number of data partitions are not sufficient enough in order to maximize the usage of available resources\n",
    "- existing number of data partitions are too heavy to be computed reliably without memory overruns\n",
    "- existing number of data partitions are too high in number such that task scheduling overhead becomes the bottleneck in the overall processing time\n",
    "\n",
    "####  Perform Aggregation/Join on a data collection\n",
    "In order to perform aggregation/join operation on data collection(s), all data records belonging to aggregation, or a join key should reside in a single data partition. If this condition is not met, data re-distribution is triggered.\n",
    "\n",
    "![repartition](images/repartition.png)\n",
    "\n",
    "\n",
    "#### Shuffle Partitions\n",
    "**The number of shuffle partitions** specifies the number of output partitions after the shuffle is executed on a data collection.\n",
    "\n",
    "**Partitioner** decides the target shuffle partition number. \n",
    "- Hash Partitioner decides the output partition based on hash code computed for key object specified for the data record.\n",
    "- Range Partitioner decides the output partition based on the comparison of key value against the range of key values estimated for each of the shuffled partition. \n",
    "\n",
    "![shuffle_partitions](images/shuffle_partitions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A shuffle block uniquely identifies a block of data which belongs to a single shuffled partition and is produced from executing shuffle write operation (by ShuffleMap task) on a single input partition during a shuffle write stage in a Spark application.\n",
    "\n",
    "![shuffle_blocks](images/shuffle_blocks.png)\n",
    "\n",
    "The unique identifier (corresponding to a shuffle block) is represented as a tuple of ShuffleId, MapId and ReduceId."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle Write\n",
    "\n",
    "Shuffle write operation is executed independently for each of the input partition which needs to be shuffled.\n",
    "\n",
    "Shuffle writers produces a **index file** and a **data file** corresponding to each of the input partition to be shuffled. \n",
    "- Index file contains locations inside data file for each of the shuffled partition \n",
    "- Data file contains actual shuffled data records ordered by shuffled partitions.\n",
    "\n",
    "![shuffle_write](images/shuffle_write.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle Read\n",
    "\n",
    "Shuffle read does pulling/fetching of those blocks from respective locations using block manager module. \n",
    "\n",
    "Finally, a sorted iterator on shuffled data records derived from fetched shuffled blocks is returned for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle Spill\n",
    "\n",
    "##### Shuffle Write Spill\n",
    "Before writing to a final index and data file, a buffer is used to store the data records (while iterating over the input partition) in order to sort the records on the basis of targeted shuffled partitions.\n",
    "\n",
    "If the memory limits of the aforesaid buffer is breached, the contents are first sorted and then spilled to disk in a temporary shuffle file.\n",
    "\n",
    "After the iteration process is over, these spilled files are again read and merged to produce the final shuffle index and data file.\n",
    "\n",
    "![shuffle_write_spill](images/shuffle_write_spill.png)\n",
    "\n",
    "##### Shuffle Read Spill\n",
    "Similar process happens on the Shuffle Read operation.\n",
    "\n",
    "![shuffle_read_spill](images/shuffle_read_spill.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Shuffle Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESS is a proxy between Executors which write shuffle blocks and Executors which read these blocks, it helps to fetch shuffle blocks.\n",
    "\n",
    "Its lifecycle is independent of a Spark Application and any of the Executors.\n",
    "\n",
    "ESS runs on each Worker Node. Executor registers on ESS and informs where it is storing the shuffle files. ESS is able to stream those files to the reading Executors.\n",
    "\n",
    "![shuffle_service](images/shuffle_service.png)\n",
    "\n",
    "Benefits:\n",
    "\n",
    "- Reliability, if an Executor dies, its shuffle files are not lost\n",
    "- Dynamic Allocation, it is mandatory to have ESS with DA\n",
    "\n",
    "#### Configuration\n",
    "\n",
    "*spark.shuffle.service.enabled* - defines if ESS is enabled\n",
    " \n",
    "*spark.shuffle.service.port* - service port\n",
    "\n",
    "*spark.shuffle.service.index.cache.size* - how big is the cache for storing Shuffle Index Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "Let's run a query and investigate different shuffle parameters in Spark UI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import functions as F, SQLContext, SparkSession, Window\n",
    "from pyspark.sql.types import*\n",
    "from random import randint\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"explore-data\")\n",
    "         .master(\"spark://spark-master:7077\")\n",
    "         .config(\"spark.eventLog.enabled\", \"true\")\n",
    "         .config(\"spark.eventLog.dir\", \"/opt/workspace/history\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate()\n",
    "         )\n",
    "\n",
    "meteo_data_file = \"data/meteo-data/parquet\"\n",
    "meteo_df = spark.read.parquet(meteo_data_file)\n",
    "observation_type_file = \"data/meteo-data/observation_type.csv\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('observation_type', StringType(), True),\n",
    "    StructField('description', StringType(), True)\n",
    "])\n",
    "\n",
    "observation_type_df = (spark.read\n",
    "               .schema(schema)\n",
    "               .option(\"header\", \"false\")\n",
    "               .csv(observation_type_file)\n",
    "              )\n",
    "\n",
    "stations_meta_file = \"data/meteo-data/stations.csv\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('station_identifier', StringType(), True),\n",
    "    StructField('latitude', FloatType(), True),\n",
    "    StructField('longitude', FloatType(), True),\n",
    "    StructField('height_above_sea_level', FloatType(), True),\n",
    "    StructField('station_name', StringType(), True)\n",
    "])\n",
    "\n",
    "stations_df = (spark.read\n",
    "               .schema(schema)\n",
    "               .option(\"header\", \"false\")\n",
    "               .csv(stations_meta_file)\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SET spark.sql.shuffle.partitions=100\")\n",
    "df2 = meteo_df.where(\"yyyy > 2015\").join(stations_df,meteo_df[\"station_identifier\"] == stations_df[\"station_identifier\"], \"inner\")\n",
    "count = df2.cache().count()\n",
    "print(count)\n",
    "df2.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is spill to disk and spill to memory? Why it's good or bad? \n",
    "2. What you need to do to minimize the spill and when it's needed? \n",
    "3. What is controlled by setting spark.sql.shuffle.partitions?\n",
    "4. What is the ideal number for this setting? \n",
    "5. Why the ShuffleWrite is significantly bigger than Data Read? \n",
    "\n",
    "To answer this question, you can change setting in pervious example and see the result\n",
    "\n",
    "Bonus question: where is shuffle stage if we remove the cache() step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

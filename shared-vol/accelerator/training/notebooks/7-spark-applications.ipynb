{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Quicksand:300,700\" />\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Fira Code\" />\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"rise.css\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Quicksand:300,700\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Fira Code\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"rise.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark applications\n",
    "\n",
    "\n",
    "![footer_logo_new](images/logo_new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So far we used techniques to explore data and execute small transformations on data to get it into the form we want.\n",
    "\n",
    "This is not the best way if we end up with transformations that need to be done on a regular basis.\n",
    "\n",
    "For this we have the `spark-submit` command.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark shell vs Spark submit\n",
    "\n",
    "#### Spark shell\n",
    "\n",
    "- Explore data\n",
    "- Try out transformations\n",
    "- Experiment different algorithms\n",
    "\n",
    "#### Spark application\n",
    "\n",
    "- Python, Scala or Java\n",
    "- For regular running jobs\n",
    "- ETL\n",
    "- Streaming applications\n",
    "- Productionized code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sparkcontext in a application\n",
    "\n",
    "#### Spark shell\n",
    "\n",
    "- Creates the spark context for you.\n",
    "\n",
    "#### Spark application\n",
    "\n",
    "- Need to create yourself (just like in the notebook).\n",
    "- Called `sc` by convention.\n",
    "- Call sc.stop when program terminates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wordcount example application\n",
    "\n",
    "Don't worry if you don't understand what's in here. We will cover it later.\n",
    "\n",
    "```python\n",
    "import argparse\n",
    "import pyspark\n",
    "import pyspark.sql.functions as sf\n",
    "import pyspark.sql.types as st\n",
    "\n",
    "\n",
    "def main(df):\n",
    "    results = (\n",
    "        df.select(sf.explode(sf.split(sf.col(\"word\"), r\"\\s+\")).alias(\"word\"))\n",
    "          .withColumn('nr', sf.lit(1))\n",
    "          .groupBy('word')\n",
    "          .agg(sf.sum('nr').alias('sum'))\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='This is a demo application.')\n",
    "    parser.add_argument('-i', '--input',  help='Input file name',  required=True)\n",
    "    parser.add_argument('-o', '--output', help='Output file name', required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    spark = (\n",
    "        pyspark.sql.SparkSession.builder\n",
    "               .getOrCreate()\n",
    "    )\n",
    "\n",
    "    schema = st.StructType([st.StructField(\"word\", st.StringType())])\n",
    "    input_df = (\n",
    "        spark.read\n",
    "             .option(\"header\", \"false\")\n",
    "             .csv(args.input, schema=schema)\n",
    "    )\n",
    "\n",
    "    counts = main(input_df)\n",
    "    counts.write.parquet(args.output)\n",
    "\n",
    "    spark.sparkContext.stop()\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Running a application\n",
    "\n",
    "```bash\n",
    "$SPARK_HOME/bin/spark-submit wordcount.py -i inputdir -o outputdir\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where do we run this\n",
    "\n",
    "Spark can run:\n",
    "\n",
    "- Locally\n",
    "- Locally with multiple threads\n",
    "- On a cluster\n",
    "    - Client mode\n",
    "    - Cluster mode\n",
    "\n",
    "The cluster option is mostly used for production jobs.  \n",
    "Local can be useful for testing and development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Client mode\n",
    "\n",
    "When using client mode:\n",
    "\n",
    " - The spark driver is part of the `spark-submit` process.\n",
    " - The `spark-submit` command does not exit until the application has finished and the driver has shut-down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cluster Mode\n",
    "\n",
    "When using cluster mode:\n",
    "\n",
    " - The spark driver runs remotely, normally in a process very similar to the workers. (The exact mechanism depends on the type of cluster.)\n",
    " - The `spark-submit` returns immediately, once the application has started, without waiting for it to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to specify the environment to run on\n",
    "\n",
    "```bash\n",
    "spark-submit --master 'local[2]' wordcount.py ....\n",
    "```\n",
    "\n",
    "Other options:\n",
    "- local\n",
    "- local[n]\n",
    "- local[*]\n",
    "- yarn (default mode is `client`)\n",
    "- yarn (if you want `cluster` mode, specify `--deploy-mode cluster`)\n",
    "- spark://HOST:PORT\n",
    "- mesos://HOST:PORT\n",
    "\n",
    "Kubernetes is new."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this chapter we looked at:\n",
    "\n",
    "+ How Spark Applications can be packaged up.\n",
    "+ The differences between running in client and cluster mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise\n",
    "Grab the code from the previous exercise. The code should do the following:\n",
    "1. First, load the heroes dataset and calculate the average attack per role using `groupBy`.\n",
    "2. Now filter the resulting averages to show only the Warrior role and print the result using `show`.\n",
    "\n",
    "Package it up in a Python file, and run it using `spark-submit` on your local machine"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

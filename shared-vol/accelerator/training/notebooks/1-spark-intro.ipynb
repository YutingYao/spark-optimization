{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Quicksand:300,700\" />\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Fira Code\" />\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"rise.css\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Quicksand:300,700\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Fira Code\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"rise.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apache Spark\n",
    "\n",
    "\n",
    "\n",
    "![footer_logo_new](images/logo_new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    " - Welcome\n",
    " - Introductions\n",
    " - Course subjects\n",
    " - The environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Welcome\n",
    "\n",
    " - Class start and end times\n",
    " - Facilities\n",
    "\n",
    "    - Wifi\n",
    "    - Restrooms\n",
    "    - Lunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Special Corona Measures\n",
    "\n",
    " - Let's try to keep webcams on, unless bandwidth becomes a problem.\n",
    " - If it's a problem, please switch on webcam if/when you have a question.\n",
    " - Please mute your microphone if you're not talking.\n",
    " - If you run into trouble and need help: let me know!\n",
    " - It's harder for me as a teacher, as I can't see you (well), bear with me :-).\n",
    " - Let's have a quick round-up of how everyone is doing :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### WiFi\n",
    "\n",
    "The access point for this class is `Xebia Guest`. The password is `EasyAccess`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introductions\n",
    "\n",
    " - About me\n",
    " - About you\n",
    "\n",
    "   - Your role\n",
    "   - Your skillset\n",
    "\n",
    "       - Data Science\n",
    "       - Hadoop\n",
    "       - Spark maybe\n",
    "       - Programming in general\n",
    "\n",
    "   - Your expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Course subjects\n",
    "\n",
    "\n",
    " - Spark basics:\n",
    "\n",
    "   - Spark Execution\n",
    "   - SparkSession\n",
    "   - DataFrames\n",
    "   - Transformation\n",
    "   - Laziness\n",
    "   - Lineage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## More Course Subjects\n",
    "\n",
    " - Spark Advanced:\n",
    "\n",
    "   - How Spark reads/writes data\n",
    "   - From DF's to Pandas DF's and back\n",
    "   - Dataframes: Basic concepts\n",
    "   - SparkSQL\n",
    "   - Narrow and wide operations (and why you should care)\n",
    "   - The Catalyst optimizer\n",
    "   - Caching and persistence\n",
    "   - More Dataframe operations\n",
    "   - Spark applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Even More Course Subjects\n",
    "\n",
    " -  DataFrames:\n",
    "\n",
    "    - Windowing operations\n",
    "    - UDF\n",
    "    - UDAFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hadoop: First Generation Big Data Platform\n",
    "\n",
    " - Solution for distributed storage and computation\n",
    " - Initially Hadoop consisted of two components:\n",
    "\n",
    "   1. HDFS: distributed filesystem\n",
    "\n",
    "      - Divide data in blocks\n",
    "      - Replicate blocks across nodes (fault-tolerance)\n",
    "      - Keep track of what is where!\n",
    "\n",
    "   2. MapReduce: distributed computation\n",
    "\n",
    "      - _Map:_ perform computation in parallel\n",
    "      - _Reduce:_ combine results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Block replication was also aimed at performance: the assumption is that we read more often than we write, and it will go faster we have multiple copies to choose from.\n",
    "\n",
    "Map-reduce also tries to place computations where the data is, so it doesn't have to cross the network (again). This is easier when there are replicas. For this to be a win, it also assumes that the code to run is smaller than the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Spark?\n",
    "\n",
    " - Spark is a second-generation Big Data platform.\n",
    " - The first generation was based on MapReduce. Powerful, but cumbersome to use with technical limitations.\n",
    " - Spark introduced a new architecture.\n",
    "\n",
    "   - Early versions presented a functional programming abstraction.\n",
    "   - The last few years have moved on towards a DataFrame/SQL abstraction.\n",
    "   - The Hadoop roots are still there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "What's different about the new architecture?\n",
    "\n",
    " - Spark tries to keep most data in memory or nearby; map-reduce passes everything via HDFS.\n",
    " - Spark is designed to work on multiple distributed platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark History\n",
    "\n",
    " - Emerged from the AMPLab at UC Berkeley, at the same time as Mesos.\n",
    " - Spark is now an Apache project.\n",
    " - It's now bundled with Cloudera, Hortonworks and MapR.\n",
    " - Available on AWS, Google Cloud and Microsoft Azure\n",
    " - Evolving very quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Spark was originally written over a weekend as a demonstration for Mesos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Data Processing\n",
    "\n",
    " - Hadoop is based on two key concepts:\n",
    "\n",
    "    - Distribute where data is stored.\n",
    "    - Run computation where the data lives. __*__\n",
    "\n",
    " - Spark adds to that:\n",
    "\n",
    "    - Provide a high level API.\n",
    "    - From the high level API automatically produce an execution plan, and optimize it where possible.\n",
    "    - Keep data in memory where possible for faster computation.\n",
    "\n",
    "__\\*__ While true for Hadoop, this is no longer *necessarily* the case for Spark jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The memory thing is often cited, and is also a bit of a lie. It turns out that Spark keeps intermediate data on disk as well. However unlike MapReduce it's usually \"local\" disk, which is faster to write to than HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Language Support\n",
    "\n",
    "Spark is implemented in Scala (a functional programming language which runs in the JVM) but supports programming in the following languages:\n",
    "\n",
    " - Scala\n",
    " - Python\n",
    " - R\n",
    " - Java"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Via Java you also get other languages like Kotlin, if that floats your boat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Common Workflows\n",
    "\n",
    "There are several common ways of working with Spark:\n",
    "\n",
    " - Notebooks, typically Jupyter\n",
    " - Spark shell (a REPL in Python or Scala)\n",
    " - Spark applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark & Jupyter Notebook\n",
    "\n",
    "Jupyter is a stand-alone Notebook environment.\n",
    "Start a Jupyter Notebook server with:\n",
    "\n",
    "```sh\n",
    "% jupyter notebook\n",
    "```\n",
    "\n",
    "This will start a notebook server and will direct your browser to it.\n",
    "\n",
    "Since Spark 2.2.0 we can do `pip install pyspark` (without needing things like `findspark`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Shell for Python\n",
    "\n",
    "Spark supports a shell, for interactive work. To launch this use the `$SPARK_HOME/bin/pyspark` command, which starts a Python REPL connected to Spark:\n",
    "\n",
    "```\n",
    "Python 3.5.3 | packaged by conda-forge | (default, Jan 24 2017, 06:45:37)\n",
    "[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.54)] on darwin\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.1.0\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.5.3 (default, Jan 24 2017 06:45:37)\n",
    "SparkSession available as 'spark'.\n",
    ">>>\n",
    ">>> sc\n",
    "<pyspark.context.SparkContext object at 0x103ba6d90>\n",
    ">>> sqlContext\n",
    "<pyspark.sql.context.SQLContext object at 0x103d7a790>\n",
    ">>> spark\n",
    "<pyspark.sql.session.SparkSession object at 0x103d7a590>\n",
    ">>> exit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Shell for Scala\n",
    "\n",
    "Spark supports a shell, for interactive work. To launch this use the `$SPARK_HOME/bin/spark-shell` command, which starts a scala REPL connected to Spark:\n",
    "\n",
    "```\n",
    "% spark-shell\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "Spark context Web UI available at http://192.168.1.114:4040\n",
    "Spark context available as 'sc' (master = local[*], app id = local-1487929648200).\n",
    "Spark session available as 'spark'.\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.1.0\n",
    "      /_/\n",
    "\n",
    "Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_112)\n",
    "Type in expressions to have them evaluated.\n",
    "Type :help for more information.\n",
    "\n",
    "scala>\n",
    "\n",
    "scala> sc\n",
    "res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@37b1218\n",
    "\n",
    "scala> spark\n",
    "res1: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@88e85ascala>sc\n",
    "\n",
    "scala> exit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this chapter we covered:\n",
    "\n",
    "- What Spark is?\n",
    "- How to start and use the Spark Shell\n",
    "- How to use Spark from within a Jupyter Notebook"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Quicksand:300,700\" />\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Fira Code\" />\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"rise.css\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Quicksand:300,700\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Fira Code\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"rise.css\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "master = 'local[2]'\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "spark = (\n",
    "    pyspark.sql.SparkSession.builder\n",
    "    .master(master)\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reading data in Spark\n",
    "\n",
    "![footer_logo_new](images/logo_new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this chapter, we'll cover the following topics:\n",
    "+ How to read data using Spark\n",
    "+ HDFS and Spark\n",
    "+ Data compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading data into dataframes\n",
    "Most input operations are under `spark.read`, for example:\n",
    "\n",
    "- `spark.read.csv()`: CSV.\n",
    "- `spark.read.json()`: JSON.\n",
    "- `spark.read.parquet()`: Parquet.\n",
    "- `spark.read.table()`: Hive table.\n",
    "\n",
    "All file-based methods with a file, a wildcard, or folder(s) with files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An example of `spark.read.csv()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_path = os.path.join(data_dir, 'chicagoCensus.csv')\n",
    "chicago = spark.read.csv(chicago_path, header=True)\n",
    "chicago.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An example of `spark.read.parquet()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "airlines_path = os.path.join(data_dir, 'airlines.parquet/')  # Folder\n",
    "airlines = spark.read.parquet(airlines_path)\n",
    "airlines.printSchema()\n",
    "# airlines.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  `read()` and locality\n",
    "\n",
    "Depending on how your run Spark, it will read files from different places:\n",
    "\n",
    "* Yarn client mode: `/data/` is the HDFS folder `/data/`.\n",
    "    * Use `file://data/` to load a local file.\n",
    "* Local mode: `/data/` is the folder `/data` on your machine.\n",
    "    * Use `hdfs://data` to load an HDFS file.\n",
    "* Any mode:\n",
    "    * Use `gs://bucket/` to load a file from Google Cloud Storage.\n",
    "    * Or `s3a://bucket/` to load a file from Amazon's S3 storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HDFS\n",
    "- Distributed filesystem:\n",
    "    - NameNode (master)\n",
    "        - Stores metadata on filesystem.\n",
    "        - Controls file permissions.\n",
    "        - Executes changes on filesystem.\n",
    "    - DataNodes\n",
    "        - Store actual data (blocks).\n",
    "        - Execute read and write operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/hdfs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why HDFS?\n",
    "\n",
    "Advantages:\n",
    "\n",
    "* Distributed load - avoid high disk/network load on single system.\n",
    "* Data locality - bring compute to the data.\n",
    "* High reliablity - replication across multiple systems.\n",
    "\n",
    "Drawbacks:\n",
    "\n",
    "* Inefficient with many small files.\n",
    "* Security, latency, ease of use, etc.\n",
    "* Combined data storage + compute = always running cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other options: blob storage\n",
    "\n",
    "Examples: Amazon S3, Google Cloud Storage.\n",
    "\n",
    "Features:\n",
    "* High durability, scalability, availability.\n",
    "* Low storage costs + additional cost options.\n",
    "* Strong support for security + auditing.\n",
    "\n",
    "Compared to HDFS: easy separation of storage and compute. But... No data locality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compression\n",
    "\n",
    "Advantages:\n",
    "* Helps reduce file sizes, especially for text-based file formats like CSV, JSON.\n",
    "* Less network load + storage costs.\n",
    "\n",
    "Drawbacks:\n",
    "* Increased CPU overhead for compressing/decompressing files.\n",
    "* Non-splittable compression formats cannot be read in blocks (requiring the entire file to be read)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Splittability\n",
    "\n",
    "Compressed files are splittable if file blocks can be read without de-compressing preceding blocks.\n",
    "\n",
    "Examples:\n",
    "* Splittable formats - bzip2/LZO\n",
    "* Non-splittable formats - gzip/snappy\n",
    "\n",
    "Note that splittability also depends on the file format: for example Avro and Parquet use compression internally and are therefore still splittable when using gzip or snappy compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this chapter, we covered:\n",
    "+ How you can read data from different formats using Spark\n",
    "+ What kind of storage you can use with Spark\n",
    "+ The tradeoffs that can be involved with compressing data to reduce storage."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

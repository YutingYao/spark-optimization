{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Quicksand:300,700\" />\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Fira Code\" />\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"rise.css\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Quicksand:300,700\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Fira Code\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"rise.css\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "master = 'local[2]'\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "spark = (\n",
    "    pyspark.sql.SparkSession.builder\n",
    "    .master(master) \n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrame Basics\n",
    "\n",
    "![footer_logo_new](images/logo_new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "1. DataFrames\n",
    "1. Basic operations\n",
    "1. Getting data out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The core concept of Spark are __DataFrames__.\n",
    "\n",
    "DataFrames are and consist of named columns containing data of a certain type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "DataFrames are similar to their counterparts in R or Python (`pandas`).\n",
    "Alternatively, you can see them as a sheet in Excel or a table in a database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Manually create a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sdf = spark.createDataFrame([[None, 'Michael'],\n",
    "                             [30, 'Andy'],\n",
    "                             [19, 'Justin'],\n",
    "                             [30, 'James Dr No From Russia with Love Bond']], \n",
    "                            schema=['age', 'name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Generally we would read data from a source.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Methods and properties\n",
    "\n",
    "DataFrames have methods and properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Actions\n",
    "\n",
    "Transformations are lazy; processing is only triggered when an action is invoked.\n",
    "\n",
    "Common actions are:\n",
    "\n",
    " - `sdf.count()`: Count the number of rows\n",
    " - `sdf.toPandas()`: Convert to a `pandas` DataFrame.\n",
    " - `sdf.show()`: Print some rows to console.\n",
    " - `sdf.collect()`: Convert to Python objects.\n",
    " - Any write action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sdf.toPandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DataSets vs DataFrames\n",
    "\n",
    "If you're using Scala, you'll get the added bonus of strong typing. If you look around, you'll notice something: a DataFrame is actually a DataSet!\n",
    "\n",
    "DataSets allow you to work with your own objects instead of the generic `Row` that we have to use in PySpark:\n",
    "\n",
    "```scala\n",
    "# Straight from a Scala sequence to a DataSet.\n",
    "case class Employee(name: String, age: Long)\n",
    "val caseClassDS = Seq(Employee(\"Amy\", 32)).toDS\n",
    "caseClassDS.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```scala\n",
    "# Convert an untyped DataFrame into a DataSet.\n",
    "case class Movie(actor_name: String,\n",
    "                 movie_title: String,\n",
    "                 produced_year: Long)\n",
    "val movies = Seq(\n",
    "    (\"Damon, Matt\", \"The Bourne Ultimatum\", 2007L),\n",
    "    (\"Damon, Matt\", \"Good Will Hunting\", 1997L)\n",
    ")\n",
    "val moviesDS = movies.toDF(\"actor_name\",\n",
    "                           \"movie_title\",\n",
    "                           \"produced_year\").as[Movie]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Basic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_path = os.path.join(data_dir, 'chicagoCensus.csv')\n",
    "chicago = spark.read.csv(chicago_path, header=True)\n",
    "chicago.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as sf\n",
    "\n",
    "chicago \\\n",
    "    .filter(sf.col('HARDSHIP INDEX').isNotNull()) \\\n",
    "    .withColumn('high_hardship', sf.col('HARDSHIP INDEX') > 20) \\\n",
    "    .groupby('high_hardship') \\\n",
    "    .agg(sf.count('*').alias('n')) \\\n",
    "    .sort('n', 'high_hardship', ascending=False) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In SQL (depending on your dialect):\n",
    "```sql\n",
    "SELECT\n",
    "    `HARDSHIP INDEX` > 20 AS high_hardship,\n",
    "    count(*) AS n\n",
    "FROM\n",
    "    chicago\n",
    "WHERE\n",
    "    `HARDSHIP INDEX` IS NOT NULL\n",
    "GROUP BY\n",
    "    `HARDSHIP INDEX` > 20\n",
    "ORDER BY n DESC\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "chicago.createOrReplaceTempView(\"chicagoTable\")\n",
    "chicagoSql = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    `HARDSHIP INDEX` > 20 AS high_hardship,\n",
    "    count(*) AS n\n",
    "FROM\n",
    "    chicagoTable\n",
    "WHERE\n",
    "    `HARDSHIP INDEX` IS NOT NULL\n",
    "GROUP BY\n",
    "    `HARDSHIP INDEX` > 20\n",
    "ORDER BY\n",
    "    n DESC\"\"\")\n",
    "chicagoSql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### (Py)Spark DataFrame API vs Spark SQL API\n",
    "\n",
    " - In many respects, the DataFrame API and the Spark SQL are equivalent:\n",
    " \n",
    "   - DataFrames represent tabular data and transformations.\n",
    "   - SQL expresses queries on tabular data.\n",
    "   \n",
    " - Sometimes you will prefer one over the other*.\n",
    " - Testing DataFrames tends to be easier than SQL.\n",
    " - In recent versions of Spark there are SQL queries that cannot be\n",
    "   implemented via the DataFrames APIs.\n",
    "   \n",
    "\\* Check out https://gdd.li/spark-df-api for more on this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "What SQL can't be done via DataFrame manipulation? There are some correlated subqueries that are only supported via SQL: I haven't been able to find a way of implementing them via DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `filter()`: Filter rows\n",
    "\n",
    "```python\n",
    ".filter(sf.col('HARDSHIP INDEX').isNotNull())\n",
    "```\n",
    "\n",
    "Filtering rows is done with booleans; all rows containg `True` will remain.\n",
    "\n",
    "Use `sf.col('column')` to refer to an existing column named `'column'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Chain complicated multiple expressions like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    chicago\n",
    "    .filter(sf.col('HARDSHIP INDEX').isNotNull() & \n",
    "            (sf.col('PER CAPITA INCOME ') > 30000) & \n",
    "            ((sf.col('Community Area Number') > 5) | ~(sf.col('HARDSHIP INDEX') > 10)))\n",
    "    .limit(2)\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Can anyone guess why all those brackets are there in the filter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Answer: Operator precedence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `withColumn()`: Adding a column\n",
    "\n",
    "```python\n",
    "sdf.withColumn('high_hardship', sf.col('HARDSHIP INDEX') > 20)\n",
    "```\n",
    "\n",
    "Add a column with `sdf.withColumn('name', expression)`.\n",
    "\n",
    "The `withColumn` function can also be used to replace an existing column in place:\n",
    "   \n",
    "```python\n",
    "sdf.withColumn(\"high_hardship\", ~sf.col(\"high_hardship\"))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `groupby().agg()`: Aggregate statistics    \n",
    " \n",
    "```python\n",
    ".groupby('high_hardship')\n",
    ".agg(sf.count('*').alias('n'))\n",
    "```\n",
    "\n",
    "* Group by one or multiple columns.\n",
    "* Aggregate with one of the aggregate functions found in `sf`.\n",
    "* Give columns a readable name by using `.alias('name')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `sort()`: Sort the DataFrame\n",
    "\n",
    "```python\n",
    ".sort('n', ascending=False)\n",
    "```\n",
    "\n",
    "* Sort the DataFrame by one or multiple columns.\n",
    "* Give a list of booleans when sorting multiple columns in different order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intermezzo on style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "All operations return a modified DataFrame.\n",
    "\n",
    "In our example, we chained all the operations under each other, while you could also write the query as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "filtered = chicago.filter(sf.col('HARDSHIP INDEX').isNotNull())\n",
    "with_hardship = filtered.withColumn('high_hardship', sf.col('HARDSHIP INDEX') > 20)\n",
    "n_per_hardship = with_hardship.groupby('high_hardship').agg(sf.count('*').alias('n'))\n",
    "sorted_n = n_per_hardship.sort('n', ascending=False)\n",
    "sorted_n.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why didn't we write our query like that?\n",
    "\n",
    " - Our first query is more readable.\n",
    " - Don't have to come up with good names for temporary variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Wide vs long chains of operations\n",
    "\n",
    "We can also operations like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago.withColumn('high_hardship', sf.col('HARDSHIP INDEX') > 20).filter(sf.col('HARDSHIP INDEX').isNotNull()).groupby('high_hardship').agg(sf.count('*').alias('n')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But that isn't really readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Irrespective of whether you're chaining long or wide, don't make them too long!\n",
    "\n",
    "While you're writing them they can be obvious but if they are too long your colleages won't be able to follow the evolving schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And they will reject your code during review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Getting data out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Similar to `spark.read`, most methods are under `sdf.write`:\n",
    "\n",
    "- `sdf.write.csv()`: CSV.\n",
    "- `sdf.write.json()`: JSON.\n",
    "- `sdf.write.parquet()`: Parquet.\n",
    "- `sdf.write.saveAsTable()`: Hive table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Save the DataFrame `sdf` as the Hive table `my_table`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf.write.saveAsTable('my_table', mode='overwrite')\n",
    "spark.table('my_table').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  `write` and locality\n",
    "\n",
    "Depending on how your run Spark, it will read files from different places.\n",
    "\n",
    "This is similar to `spark.read.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `write` and tasks\n",
    "\n",
    "Spark runs many tasks for a single stage (more about that later). They will run in parallel (depending on the number of cores per executor).\n",
    "\n",
    "Each task will output a part of the file when done.\n",
    "\n",
    "<img src=\"images/partitioning.png\" width=\"70%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you want to avoid this, use `repartition()` or `coalesce()` before writing:\n",
    "\n",
    "- `.repartition(numPartitions, *cols)`: Scales the number of partitions up or down, optionally distributing the data so that rows with the same values in the specified columns end up in the same partion.\n",
    "- `.coalesce(numPartitions)`: Scales the number of partitions down.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     sdf\n",
    "#     .repartition('name')\n",
    "#     .write.saveAsTable('my_second_table')\n",
    "# )\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Minimizing data movement is important. If you have the following situation:\n",
    "\n",
    "- Node1: Partition 1A, 1B, 1C\n",
    "- Node2: Partition 2A, 2B, 2C\n",
    "- Node3: Partition 3A, 3B, 3C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Invoking `.coalesce(3)` will produce:\n",
    "\n",
    "- Node1: Partition 1\n",
    "- Node2: Partition 2\n",
    "- Node3: Partition 3\n",
    "\n",
    "Invoking `.coalesce(2)` will produce:\n",
    "\n",
    "- Node1: Partition (1 + 3A)\n",
    "- Node2: Partition (2 + 3(B+C))\n",
    "\n",
    "The parenthesis indicate a single partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `coalesce` vs `repartition`\n",
    "We can use `coalesce` and `repartition` to perform (essentially) the same task. I can run both of these commands with the same effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.repartition(1).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.coalesce(1).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, there is a difference.\n",
    "\n",
    "Can anyone guess what it is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`repartition` will __always__ incur a *full* shuffle of the data, regardless of whether one is necessary.\n",
    "\n",
    "`coalesce` will try to combine partitions in order to get to the desired number of partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Rule of thumb: `coalesce` vs `repartition`\n",
    "\n",
    "Typically, only use `repartition` when the future number of partitions is greater than the current number, or if you are looking to partition by a (set of) column(s). Otherwise, use `coalesce`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1. Load the Heroes of the Storm dataset with `spark.read.csv()`.\n",
    "   Make sure your parse the `headers` in the first row.\n",
    "1. Check the dtypes: what do you notice?\n",
    "   How can you let Spark infer the schema?\n",
    "1. Explore the data: some _NaN_ values are not encoded properly.\n",
    "   Tell `spark.read.csv` how _NaN_ is encoded.\n",
    "1. Filter out the hero with the _NaN_ value.\n",
    "1. Which hero has the most hp?\n",
    "1. Add a column with the `attack_momentum`, computed as: ${attack} * {attack\\_spd}$.\n",
    "1. Which role on average has the highest attack?\n",
    "1. Figure out which roles and types of attack frequently occur together.\n",
    "1. Deliver a dataframe with names of the heroes with the highest attack in their role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heroes_path = os.path.join(data_dir, 'heroes.csv')\n",
    "heroes = spark.read.csv(heroes_path, header=True, inferSchema=True, nanValue='NA').filter(~sf.isnan('attack'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heroes.sort('attack', ascending=False).groupBy('role').agg(sf.first('name')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Bonus\n",
    "1. Make a function that accepts a dataframe and a list colnames. \n",
    "Let it return the mean and standard deviation of the columns.\n",
    "1. Apply the function to the hp and attack column such that the result has columns:\n",
    "    - `hp_mean`\n",
    "    - `hp_stddev`\n",
    "    - `attack_mean`\n",
    "    - `attack_stddev`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%load \"../answers/02_heroes.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this chapter, we looked at:\n",
    "+ The basics of creating, filtering and working with Dataframes.\n",
    "+ How to write dataframes to disk."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

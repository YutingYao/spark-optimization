{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Quicksand:300,700\" />\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Fira Code\" />\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"rise.css\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Quicksand:300,700\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://fonts.googleapis.com/css?family=Fira Code\" />\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"rise.css\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "master = 'local[2]'\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "import pyspark.sql.functions as sf\n",
    "import pyspark.sql.types as st\n",
    "\n",
    "spark = (\n",
    "    pyspark.sql.SparkSession.builder\n",
    "    .master(master) \n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# DataFrames Advanced\n",
    "1. More DataFrame operations\n",
    "1. Joins\n",
    "1. Columns\n",
    "1. Functions\n",
    "1. UDFs & UDAFs\n",
    "\n",
    "![footer_logo_new](images/logo_new.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. More DataFrame operations\n",
    "\n",
    "We only covered some of the DataFrame operations.\n",
    "\n",
    "There are many more, such as:\n",
    "\n",
    "- `drop()`\n",
    "- `select()`\n",
    "- `join()`\n",
    "- `limit()`\n",
    "- `distinct()`\n",
    "- `drop_duplicates()`\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Create a simple DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1 = spark.createDataFrame([[1, 'a'], [2, 'b'], [2, 'c'], [3, 'd']],\n",
    "                             schema=['number', 'letter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `select()`: Select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1.select('number', 'letter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `drop()`: Drop columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1.drop('letter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `join()`: Join two DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sdf2 = spark.createDataFrame([[2], [3], [4]], schema=['number'])\n",
    "sdf2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Inner join on a column present in both DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sdf1.join(sdf2, on='number', how='inner').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Left-join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sdf1.join(sdf2, on=(sdf1.number >= sdf2.number), \n",
    "              how='left')\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that here we end up with the columns from both DataFrames.\n",
    "\n",
    "To drop it the column from `sdf1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    sdf1.join(sdf2, on=(sdf1.number >= sdf2.number), \n",
    "              how='left')\n",
    "    .drop(sdf1.number)\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Spark supports quite a few join types:\n",
    "+ inner\n",
    "+ left_outer\n",
    "+ left_anti\n",
    "+ right_outer\n",
    "+ full_outer\n",
    "+ left_semi\n",
    "\n",
    "Try them out on the datasets shown, and formulate what each of them does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `limit()`: Limit the number of rows\n",
    "\n",
    "Crucial when you're doing a `.toPandas()`.\n",
    "\n",
    "Your big Spark DataFrame often won't fit in the RAM of your Gateway, so you don't want all rows to be converted.\n",
    "\n",
    "Alternatively, use `sample()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sdf1.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sidebar: limit() without order() gives arbitrary results\n",
    "\n",
    "Limit chooses the first $n$ rows in the DataFrame; if you haven't explicitly specified the order of the DataFrame, what's it going to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Undefined behaviour is something your colleagues will reject during code review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Something to think about: can the combination of `order()` and `limit()` be optimized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `limit()` relative: `sample()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want a more representative sample of your data: `sample(withReplacement, fraction, seed=None)`\n",
    "\n",
    "Note that you need to come up with a good number for `fraction`, as you can't directly specify the number of samples you want. You can however combine `sample()` with `limit()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `distinct()`: Find distinct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sdf1\n",
    "    .select('number')\n",
    "    .distinct()\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `drop_duplicates()`: Drop duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sdf3 = (\n",
    "    sdf1\n",
    "    .drop_duplicates(subset=['number'])\n",
    ")\n",
    "\n",
    "sdf3.show()\n",
    "\n",
    "sdf1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Columns\n",
    "\n",
    "`sf.col()` is an important tool. Use it to create new columns from mathematical operations; filter rows; etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's get some new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = spark.createDataFrame(\n",
    "    [[float('nan'), 'John'],\n",
    "     [None, 'Michael'],\n",
    "     [30., 'Andy'],\n",
    "     [19., 'Justin'],\n",
    "     [30., 'James Dr No From Russia with Love Bond']], \n",
    "    schema = ['age', 'name']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons.filter(sf.col('name') == 'Andy').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons.filter(sf.col('name') != 'Andy').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `.isin()`: Searching in a list\n",
    "\n",
    "2 ways of being Andy or Justin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    persons\n",
    "    .withColumn('is_andy_or_justin', ((sf.col('name') == 'Andy') |\n",
    "                                      (sf.col('name') == 'Justin')))\n",
    "    .withColumn('is_andy_or_justin2', sf.col('name').isin('Andy', 'Justin', 'John', 'Jack'))\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Use `sf.col('name').isin()` when having many alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "teen_ages = list(range(10, 20))\n",
    "teens = persons.withColumn('is_teen', sf.col('age').isin(teen_ages))\n",
    "teens.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `~`: Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    teens.withColumn('aint_no_teen', ~sf.col('is_teen'))\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `.isNull()` and `.isNotNull()`: Finding missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    teens\n",
    "    .withColumn('missing_age', sf.col('age').isNull())\n",
    "    .withColumn('not_missing_age', sf.col('age').isNotNull())\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `.startswith()` and `.contains()`: String operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    teens\n",
    "    .withColumn('starts_with_J', sf.col('name').startswith('J'))\n",
    "    .withColumn('has_an_a', sf.col('name').contains('a'))\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A lot of functionality can be found in the module `pyspark.sql.functions` or are methods of `sf.col('name')`.\n",
    "\n",
    "* Needed for basic operations.\n",
    "* Lots of functions (too many).\n",
    "* Read the [API docs](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `sf.when().otherwise()`: case statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    persons\n",
    "    .withColumn('whos_this', \n",
    "                sf.when(sf.col('name') == 'Andy', 'Yup, Andy')\n",
    "                  .when(sf.col('name') == 'Justin', 'Justin here')\n",
    "                  .otherwise('No idea'))\n",
    "     .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Adding boolean columns, don't do it with `sf.when`, just add it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    persons\n",
    "    .withColumn('is_andy_good', sf.col('name') == 'Andy') # YES\n",
    "    .withColumn('is_andy_bad',\n",
    "                sf.when(sf.col('name') == 'Andy', True)\n",
    "                  .otherwise(False)) # NO\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `sf.lit()`: Add a constant column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "persons.withColumn('value', sf.lit(5)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `sf.isnull()`, `sf.isnan()`: Find missing or non-numbers\n",
    "\n",
    "- Missing is something different than Not a Number\n",
    "- (Columns also have the methods `.isNull()` and `isNotNull()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    persons\n",
    "    .withColumn('is_missing', sf.isnull('age'))\n",
    "    .withColumn('is_nan', sf.isnan('age'))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that we can't see the difference between NaN and missing in `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    persons\n",
    "    .withColumn('is_missing', sf.isnull('age'))\n",
    "    .withColumn('is_nan', sf.isnan('age'))\n",
    "    .toPandas()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise\n",
    "Recall our heroes dataset:\n",
    "1. Sometimes our heroes like to fight in pairs. Find the most powerful pair, based on their cumulative HP. We place a few restrictions:\n",
    "    - A pair must contain different roles\n",
    "    - A pair A-B is the same as a pair B-A. Each pair should only appear once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heroes_path = os.path.join(data_dir, 'heroes.csv')\n",
    "heroes = (\n",
    "    spark.read.csv(heroes_path, header=True, inferSchema=True, nanValue='NA')\n",
    "    .filter(~sf.isnan('attack'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%load ../answers/02_heroes_pairs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. UDFs & UDAFs\n",
    "\n",
    "User defined functions (UDFs) allow you to write Python code that gets executed on every row. User defined aggregate functions (UDAFs) allow you to write Python code that creates aggregates over all (or multiple) rows in a DataFrame.\n",
    "\n",
    "Since Spark 2.3 the whole machinery can use Arrow, an in-memory format for analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# double because we accept a Series of doubles. SCALAR because we give back a Series of doubles\n",
    "# alternatives are GROUPED_MAP, that gives back a dataframe\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)  \n",
    "def plus_one(v):\n",
    "    return v + 1\n",
    "    \n",
    "(\n",
    "    spark.range(0, 10 * 1000 * 1000)\n",
    "    .withColumn('plus_one', plus_one(sf.col('id')))\n",
    "    .select(sf.sum(sf.col('plus_one')))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is a slower alternative (that you shouldn't use, unless things gets crashy!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "len_plus_one_udf = sf.udf(lambda v: v + 1, st.IntegerType())\n",
    "\n",
    "(\n",
    "    spark.range(0, 10 * 1000 * 1000)\n",
    "    .withColumn('plus_one', len_plus_one_udf(sf.col('id')))\n",
    "    .select(sf.sum(sf.col('plus_one')))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "def my_f(el):\n",
    "    return el + 1\n",
    "\n",
    "@sf.udf(st.IntegerType())\n",
    "def my_g(el):\n",
    "    return my_f(el) - 1\n",
    "\n",
    "(\n",
    "    spark.range(0, 10 * 1000 * 1000)\n",
    "    .withColumn('neutral', my_g(sf.col('id')))\n",
    "    .select(sf.sum(sf.col('neutral')))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this example there's also the Spark native way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "(\n",
    "    spark.range(0, 10 * 1000 * 1000)\n",
    "    .withColumn('plus_one', sf.col('id') + 1)\n",
    "    .select(sf.sum(sf.col('plus_one')))\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It's fastest by a long way. Anyone want to guess why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since Spark 2.3 it is also possible to apply user-defined Pandas functions on groups resulting from `.groupby()`. The resulting dataframe can be of arbitrary length. The function type of the `@pandas_udf` should be `PandasUDFType.GROUPED_MAP`.\n",
    "\n",
    "This allows you to define aggregate functions over groups (or the entire dataframe):\n",
    "\n",
    "```python\n",
    "sdf.groupby(column).apply(pandas_apply_udf)\n",
    "```\n",
    "\n",
    "Note! Be careful with your `pyarrow` versions. Spark may not always work with the latest version. If in doubt, check the versions in use by Databricks [here](https://docs.databricks.com/release-notes/runtime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "df = spark.createDataFrame(\n",
    "    [(1, 1.0),\n",
    "     (1, 2.0), \n",
    "     (2, 3.0),\n",
    "     (2, 5.0),\n",
    "     (2, 10.0)],\n",
    "    (\"id\", \"v\"))\n",
    "\n",
    "@pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)\n",
    "def my_funky_aggregate(v):\n",
    "    return v.mean() - v.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg(my_funky_aggregate(df['v'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(df['id']).agg(my_funky_aggregate(df['v'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "In this chapter we looked at:\n",
    "+ Some useful functions to manipulate Spark dataframes\n",
    "+ Joins across dataframes\n",
    "+ The different types of UDF's and UDAF's and how to define them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercises (part 1)\n",
    "\n",
    "1. Explore the `airlines` DataFrame, and count how many NaN's you have in each column;\n",
    "1. Fill the NaN with something that makes sense for each column.\n",
    "1. Capture the state in the `airport_name` column (e.g. 'NY' in 'New York, NY: John F. Kennedy International') by using `sf.split()` and `sf.col('name').getItem()` twice.\n",
    "1. Do the same, but now with `sf.regexp_extract`.\n",
    "1. Do the same, but now with a pandas_udf.\n",
    "1. Make a new dataframe `airport_states` with columns `airport` and `state`.\n",
    "1. Remove duplicates from sdf_states (hint: lookup `drop_duplicates()` in the docs).\n",
    "1. Join `airport_states` onto the original `airports`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines_path = os.path.join(data_dir, 'airlines.parquet')\n",
    "airlines = spark.read.parquet(airlines_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercises (part 2)\n",
    "9. Add a column `weather_condition` that is: \n",
    "```\n",
    "'rainy' if the `weather_delay` is greather than 1200\n",
    "'stormy' if in addition to this the arrival is diverted by more than 15 minutes\n",
    "'bright' otherwise\n",
    "```\n",
    "10. Split the DataFrame into a train and test set sorted by time cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%load ../answers/02_airlines.py"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

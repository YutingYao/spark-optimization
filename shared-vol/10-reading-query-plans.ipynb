{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading query plans\n",
    "\n",
    "![footer_logo_new](images/logo_new.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import functions as F, SQLContext, SparkSession, Window\n",
    "from pyspark.sql.types import*\n",
    "from random import randint\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"workshop-spark-optimisation\")\n",
    "         .master(\"spark://spark-master:7077\")\n",
    "         .config(\"spark.eventLog.enabled\", \"true\")\n",
    "         .config(\"spark.eventLog.dir\", \"/opt/workspace/history\")\n",
    "         .config(\"spark.speculation\", \"true\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate()\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meteo observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_identifier: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- observation_type: string (nullable = true)\n",
      " |-- observation_value: integer (nullable = true)\n",
      " |-- MFLAG1: string (nullable = true)\n",
      " |-- QFLAG1: string (nullable = true)\n",
      " |-- SFLAG1: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- yyyy: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meteo_data_file = \"data/meteo-data/parquet\"\n",
    "meteo_df = spark.read.parquet(meteo_data_file)\n",
    "meteo_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- station_identifier: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- height_above_sea_level: float (nullable = true)\n",
      " |-- station_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stations_meta_file = \"data/meteo-data/stations.csv\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('station_identifier', StringType(), True),\n",
    "    StructField('latitude', FloatType(), True),\n",
    "    StructField('longitude', FloatType(), True),\n",
    "    StructField('height_above_sea_level', FloatType(), True),\n",
    "    StructField('station_name', StringType(), True)\n",
    "])\n",
    "\n",
    "stations_df = (spark.read\n",
    "               .schema(schema)\n",
    "               .option(\"header\", \"false\")\n",
    "               .csv(stations_meta_file)\n",
    "              )\n",
    "stations_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist a df to speed up a demo and also a good example if you need to use df multiple times \n",
    "df1 = (meteo_df\n",
    "       .where(\"yyyy >=2018\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.join(stations_df,meteo_df[\"station_identifier\"] == stations_df[\"station_identifier\"], \"inner\")\n",
    "df2 = df2.select(df2['observation_type'], df2['observation_value'], df2['station_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180983588"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Project (10)\n",
      "+- * BroadcastHashJoin Inner BuildRight (9)\n",
      "   :- * Project (4)\n",
      "   :  +- * Filter (3)\n",
      "   :     +- * ColumnarToRow (2)\n",
      "   :        +- Scan parquet  (1)\n",
      "   +- BroadcastExchange (8)\n",
      "      +- * Project (7)\n",
      "         +- * Filter (6)\n",
      "            +- Scan csv  (5)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [4]: [station_identifier#0, observation_type#2, observation_value#3, yyyy#8]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/opt/workspace/data/meteo-data/parquet]\n",
      "PartitionFilters: [isnotnull(yyyy#8), (yyyy#8 >= 2018)]\n",
      "PushedFilters: [IsNotNull(station_identifier)]\n",
      "ReadSchema: struct<station_identifier:string,observation_type:string,observation_value:int>\n",
      "\n",
      "(2) ColumnarToRow [codegen id : 2]\n",
      "Input [4]: [station_identifier#0, observation_type#2, observation_value#3, yyyy#8]\n",
      "\n",
      "(3) Filter [codegen id : 2]\n",
      "Input [4]: [station_identifier#0, observation_type#2, observation_value#3, yyyy#8]\n",
      "Condition : isnotnull(station_identifier#0)\n",
      "\n",
      "(4) Project [codegen id : 2]\n",
      "Output [3]: [station_identifier#0, observation_type#2, observation_value#3]\n",
      "Input [4]: [station_identifier#0, observation_type#2, observation_value#3, yyyy#8]\n",
      "\n",
      "(5) Scan csv \n",
      "Output [2]: [station_identifier#18, station_name#22]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/opt/workspace/data/meteo-data/stations.csv]\n",
      "PushedFilters: [IsNotNull(station_identifier)]\n",
      "ReadSchema: struct<station_identifier:string,station_name:string>\n",
      "\n",
      "(6) Filter [codegen id : 1]\n",
      "Input [2]: [station_identifier#18, station_name#22]\n",
      "Condition : isnotnull(station_identifier#18)\n",
      "\n",
      "(7) Project [codegen id : 1]\n",
      "Output [2]: [station_identifier#18, station_name#22]\n",
      "Input [2]: [station_identifier#18, station_name#22]\n",
      "\n",
      "(8) BroadcastExchange\n",
      "Input [2]: [station_identifier#18, station_name#22]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, true])), [id=#102]\n",
      "\n",
      "(9) BroadcastHashJoin [codegen id : 2]\n",
      "Left keys [1]: [station_identifier#0]\n",
      "Right keys [1]: [station_identifier#18]\n",
      "Join condition: None\n",
      "\n",
      "(10) Project [codegen id : 2]\n",
      "Output [3]: [observation_type#2, observation_value#3, station_name#22]\n",
      "Input [5]: [station_identifier#0, observation_type#2, observation_value#3, station_identifier#18, station_name#22]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.explain(mode='formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\n",
      "Project [observation_type#2, observation_value#3, station_name#22], Statistics(sizeInBytes=15.3 PiB)\n",
      "+- Join Inner, (station_identifier#0 = station_identifier#18), Statistics(sizeInBytes=27.0 PiB)\n",
      "   :- Project [station_identifier#0, observation_type#2, observation_value#3], Statistics(sizeInBytes=2.9 GiB)\n",
      "   :  +- Filter ((isnotnull(yyyy#8) AND (yyyy#8 >= 2018)) AND isnotnull(station_identifier#0)), Statistics(sizeInBytes=7.8 GiB)\n",
      "   :     +- Relation[station_identifier#0,date#1,observation_type#2,observation_value#3,MFLAG1#4,QFLAG1#5,SFLAG1#6,time#7,yyyy#8] parquet, Statistics(sizeInBytes=7.8 GiB)\n",
      "   +- Project [station_identifier#18, station_name#22], Statistics(sizeInBytes=9.3 MiB)\n",
      "      +- Filter isnotnull(station_identifier#18), Statistics(sizeInBytes=11.6 MiB)\n",
      "         +- Relation[station_identifier#18,latitude#19,longitude#20,height_above_sea_level#21,station_name#22] csv, Statistics(sizeInBytes=11.6 MiB)\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) Project [observation_type#2, observation_value#3, station_name#22]\n",
      "+- *(2) BroadcastHashJoin [station_identifier#0], [station_identifier#18], Inner, BuildRight\n",
      "   :- *(2) Project [station_identifier#0, observation_type#2, observation_value#3]\n",
      "   :  +- *(2) Filter isnotnull(station_identifier#0)\n",
      "   :     +- *(2) ColumnarToRow\n",
      "   :        +- FileScan parquet [station_identifier#0,observation_type#2,observation_value#3,yyyy#8] Batched: true, DataFilters: [isnotnull(station_identifier#0)], Format: Parquet, Location: InMemoryFileIndex[file:/opt/workspace/data/meteo-data/parquet], PartitionFilters: [isnotnull(yyyy#8), (yyyy#8 >= 2018)], PushedFilters: [IsNotNull(station_identifier)], ReadSchema: struct<station_identifier:string,observation_type:string,observation_value:int>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])), [id=#102]\n",
      "      +- *(1) Project [station_identifier#18, station_name#22]\n",
      "         +- *(1) Filter isnotnull(station_identifier#18)\n",
      "            +- FileScan csv [station_identifier#18,station_name#22] Batched: false, DataFilters: [isnotnull(station_identifier#18)], Format: CSV, Location: InMemoryFileIndex[file:/opt/workspace/data/meteo-data/stations.csv], PartitionFilters: [], PushedFilters: [IsNotNull(station_identifier)], ReadSchema: struct<station_identifier:string,station_name:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.explain(mode='cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints the Query Plan. \n",
    "\n",
    "Allows to specify options like mode: 'formatted', 'cost', 'codegen'. Formatted is especially useful, it breaks down spagetti into something meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to read text output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding tree structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you see a tree structured output like this:\n",
    "```\n",
    "== Physical Plan ==\n",
    "* SortMergeJoin Inner (12)\n",
    ":- * Sort (6)\n",
    ":  +- Exchange (5)\n",
    ":     +- * Project (4)\n",
    ":        +- * Filter (3)\n",
    ":           +- * ColumnarToRow (2)\n",
    ":              +- Scan parquet  (1)\n",
    "+- * Sort (11)\n",
    "   +- Exchange (10)\n",
    "      +- * Project (9)\n",
    "         +- * Filter (8)\n",
    "            +- Scan csv  (7)\n",
    "```         \n",
    "\n",
    "You should read every branch from the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example processing data from parquet source:\n",
    "```\n",
    ":- * Sort (6)\n",
    ":  +- Exchange (5)\n",
    ":     +- * Project (4)\n",
    ":        +- * Filter (3)\n",
    ":           +- * ColumnarToRow (2)\n",
    ":              +- Scan parquet  (1)\n",
    "```\n",
    "this part of the query does:\n",
    "1. Scan parquet\n",
    "2. Converts columnar format to rows\n",
    "3. Filters based on the predicate\n",
    "4. Projects (selects) only required columns\n",
    "5. Does the exchange, shuffle\n",
    "6. Sorts the shuffled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually two branches are a part of the `SortMergeJoin Inner` operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the tree structure, there is a corresponding text explanation:\n",
    "\n",
    "```\n",
    "(8) Filter [codegen id : 3]\n",
    "Input [5]: [station_identifier#449, latitude#450, longitude#451, height_above_sea_level#452, station_name#453]\n",
    "Condition : isnotnull(station_identifier#449)\n",
    "\n",
    "(9) Project [codegen id : 3]\n",
    "Output [5]: [station_identifier#449, latitude#450, longitude#451, height_above_sea_level#452, station_name#453]\n",
    "Input [5]: [station_identifier#449, latitude#450, longitude#451, height_above_sea_level#452, station_name#453]\n",
    "```\n",
    "\n",
    "Where `(8) Filter` is the name and id of the operator. And `[codegen id : 3]` is id of the codegen block, which incapsulates this Filter operator. See codegen below.\n",
    "\n",
    "Input and Output represent the columns which are used by the operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codegen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you see Codegen, it indicates that Spark query plan has generated a single block of operations. This block is combined from transformations, which could be applied on a single executor without a shuffle. \n",
    "\n",
    "Codegens are great because they take your code as an input and rewrite it as a single Spark-native code. \n",
    "\n",
    "In the Spark UI's SQL tab you can see a Codegen as a `WholeStageCodegen` blue box, which includes other smaller boxes. That's the visualization of the merged transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![codegen](images/codegen.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scan operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parquet\n",
    "In this example we observe Parquet reading stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(1) Scan parquet \n",
    "Output [4]: [station_identifier#0, observation_type#2, observation_value#3, yyyy#8]\n",
    "Batched: true\n",
    "Location: InMemoryFileIndex [file:/opt/workspace/data/meteo-data/parquet]\n",
    "PartitionFilters: [isnotnull(yyyy#8), (yyyy#8 >= 2018)]\n",
    "PushedFilters: [IsNotNull(station_identifier)]\n",
    "ReadSchema: struct<station_identifier:string,observation_type:string,observation_value:int>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pushdown Projection\n",
    "If you look at the `Output`, you can see the list of columns, which are needed for the query. This list is pushed by the Query Optimizer because not all of the columns are needed for the end result.\n",
    "\n",
    "#### Filters\n",
    "\n",
    "##### PartitionFilters\n",
    "`yyyy >= 2018` is pushed to the source, so that only files in the relevant partitions will be accessed.\n",
    "\n",
    "##### PushedFilters\n",
    "`IsNotNull(station_identifier)` - this column will participate in the inner join operation, thus cannot be null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![scan_parquet_csv](images/scan_parquet_csv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark UI shows some of the statistical runtime information, for example number of files, size etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV\n",
    "`Scan csv` shows the stage of reading CSV file(s). Because it is not partitioned (and there is no CSV partitioning) you can see the scan of a single file with some runtime information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast Hash Join\n",
    "\n",
    "Type of join, when one of the tables is small enough to be distributed to each of the executor and make join with each of the partition.\n",
    "\n",
    "![broadcast_hash_join](images/broadcast_hash_join.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashAggregate\n",
    "\n",
    "Is the type of exchange when an aggregation happens first on each of the partitions, and then is transfered to the Driver for the final aggregation.\n",
    "\n",
    "In our example `count()` operation consists of 3 intermediate counts on executors, values are send over the network (exchange) and the last aggregate count on the Driver.\n",
    "\n",
    "![hash_aggregate](images/hash_aggregate.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. Try to run the following query and explain the query plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "* Project (12)\n",
      "+- * BroadcastHashJoin LeftOuter BuildRight (11)\n",
      "   :- * Project (6)\n",
      "   :  +- BatchEvalPython (5)\n",
      "   :     +- * Project (4)\n",
      "   :        +- * Filter (3)\n",
      "   :           +- * ColumnarToRow (2)\n",
      "   :              +- Scan parquet  (1)\n",
      "   +- BroadcastExchange (10)\n",
      "      +- * Project (9)\n",
      "         +- * Filter (8)\n",
      "            +- Scan csv  (7)\n",
      "\n",
      "\n",
      "(1) Scan parquet \n",
      "Output [3]: [station_identifier#82, observation_type#84, yyyy#90]\n",
      "Batched: true\n",
      "Location: InMemoryFileIndex [file:/opt/workspace/data/meteo-data/parquet]\n",
      "PartitionFilters: [isnotnull(yyyy#90), (yyyy#90 >= 2000)]\n",
      "PushedFilters: [IsNotNull(station_identifier), EqualTo(station_identifier,AG000060390)]\n",
      "ReadSchema: struct<station_identifier:string,observation_type:string>\n",
      "\n",
      "(2) ColumnarToRow [codegen id : 1]\n",
      "Input [3]: [station_identifier#82, observation_type#84, yyyy#90]\n",
      "\n",
      "(3) Filter [codegen id : 1]\n",
      "Input [3]: [station_identifier#82, observation_type#84, yyyy#90]\n",
      "Condition : (isnotnull(station_identifier#82) AND (station_identifier#82 = AG000060390))\n",
      "\n",
      "(4) Project [codegen id : 1]\n",
      "Output [2]: [station_identifier#82, observation_type#84]\n",
      "Input [3]: [station_identifier#82, observation_type#84, yyyy#90]\n",
      "\n",
      "(5) BatchEvalPython\n",
      "Input [2]: [station_identifier#82, observation_type#84]\n",
      "Arguments: [addStationUDF(station_identifier#82)], [pythonUDF0#111]\n",
      "\n",
      "(6) Project [codegen id : 3]\n",
      "Output [2]: [pythonUDF0#111 AS standard_station_identifier#104, observation_type#84]\n",
      "Input [3]: [station_identifier#82, observation_type#84, pythonUDF0#111]\n",
      "\n",
      "(7) Scan csv \n",
      "Output [2]: [observation_type#100, description#101]\n",
      "Batched: false\n",
      "Location: InMemoryFileIndex [file:/opt/workspace/data/meteo-data/observation_type.csv]\n",
      "PushedFilters: [IsNotNull(observation_type)]\n",
      "ReadSchema: struct<observation_type:string,description:string>\n",
      "\n",
      "(8) Filter [codegen id : 2]\n",
      "Input [2]: [observation_type#100, description#101]\n",
      "Condition : isnotnull(observation_type#100)\n",
      "\n",
      "(9) Project [codegen id : 2]\n",
      "Output [2]: [observation_type#100, description#101]\n",
      "Input [2]: [observation_type#100, description#101]\n",
      "\n",
      "(10) BroadcastExchange\n",
      "Input [2]: [observation_type#100, description#101]\n",
      "Arguments: HashedRelationBroadcastMode(List(input[0, string, true])), [id=#163]\n",
      "\n",
      "(11) BroadcastHashJoin [codegen id : 3]\n",
      "Left keys [1]: [observation_type#84]\n",
      "Right keys [1]: [observation_type#100]\n",
      "Join condition: None\n",
      "\n",
      "(12) Project [codegen id : 3]\n",
      "Output [3]: [observation_type#84, standard_station_identifier#104, description#101]\n",
      "Input [4]: [standard_station_identifier#104, observation_type#84, observation_type#100, description#101]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import functions as F, SQLContext, SparkSession, Window\n",
    "from pyspark.sql.types import*\n",
    "from random import randint\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"explore-data\")\n",
    "         .master(\"spark://spark-master:7077\")\n",
    "         .config(\"spark.eventLog.enabled\", \"true\")\n",
    "         .config(\"spark.eventLog.dir\", \"/opt/workspace/history\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate()\n",
    "         )\n",
    "\n",
    "meteo_data_file = \"data/meteo-data/parquet\"\n",
    "meteo_df = spark.read.parquet(meteo_data_file)\n",
    "observation_type_file = \"data/meteo-data/observation_type.csv\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('observation_type', StringType(), True),\n",
    "    StructField('description', StringType(), True)\n",
    "])\n",
    "\n",
    "observation_type_df = (spark.read\n",
    "               .schema(schema)\n",
    "               .option(\"header\", \"false\")\n",
    "               .csv(observation_type_file)\n",
    "              )\n",
    "\n",
    "def addStationUDF(s):\n",
    "  if s is None :\n",
    "    return None\n",
    "  else :\n",
    "    return s + ' station'\n",
    "spark.udf.register(\"addStationUDF\", addStationUDF)\n",
    "\n",
    "meteo_df.registerTempTable(\"meteo_table\")\n",
    "\n",
    "df_udf = spark.sql(\"\"\"\n",
    "                    select \n",
    "                       addStationUDF(station_identifier) as standard_station_identifier\n",
    "                       , observation_type\n",
    "                    from \n",
    "                        meteo_table\n",
    "                    where station_identifier = 'AG000060390'\n",
    "                    and yyyy >= 2000\n",
    "                \"\"\")\n",
    "res = df_udf.join(observation_type_df, \"observation_type\", \"left\")\n",
    "\n",
    "res.explain(mode='formatted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What does BatchEvalPython block represents? \n",
    "3. Where you can find query plan? \n",
    "4. How to get a query plan for particular stage? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
